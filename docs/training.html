<!DOCTYPE html>
<html lang="en" data-accent-color="plum" data-content_root="./">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Training - mlcg 0.1.3 documentation</title><link rel="index" title="Index" href="genindex.html" /><link rel="search" title="Search" href="search.html" /><link rel="next" title="Simulations" href="simulation/index.html" /><link rel="prev" title="Loss Functions" href="models/losses.html" /><script>
    function setColorMode(t){let e=document.documentElement;e.setAttribute("data-color-mode",t);let a=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,s=t;"auto"===t&&(s=a?"dark":"light"),"light"===s?(e.classList.remove("dark"),e.classList.add("light")):(e.classList.remove("light"),e.classList.add("dark"))}
    setColorMode(localStorage._theme||"auto");
  </script><link rel="stylesheet" type="text/css" href="_static/pygments.css?v=31853864" />
    <link rel="stylesheet" type="text/css" href="_static/shibuya.css?v=44020203" />
    <link media="print" rel="stylesheet" type="text/css" href="_static/print.css?v=20ff2c19" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
<style>
:root {
  --sy-f-text: "Inter", var(--sy-f-sys), var(--sy-f-cjk), sans-serif;
  --sy-f-heading: "Inter", var(--sy-f-sys), var(--sy-f-cjk), sans-serif;
}
</style>
    <meta property="og:type" content="website"/><meta property="og:title" content="Training"/>
    <meta name="twitter:card" content="summary"/>
  </head>
<body><div class="sy-head">
  <div class="sy-head-blur"></div>
  <div class="sy-head-inner sy-container mx-auto">
    <a class="sy-head-brand" href="index.html">
      
      
      <strong>mlcg</strong>
    </a>
    <div class="sy-head-nav" id="head-nav">
      <nav class="sy-head-links"></nav>
      <div class="sy-head-extra flex items-center print:hidden"><form class="searchbox flex items-center" action="search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <kbd>/</kbd>
</form><div class="sy-head-socials"></div></div>
    </div>
    <div class="sy-head-actions flex items-center shrink-0 print:hidden"><button class="js-theme theme-switch flex items-center"
data-aria-auto="Switch to light color mode"
data-aria-light="Switch to dark color mode"
data-aria-dark="Switch to auto color mode">
<i class="i-lucide theme-icon"></i>
</button><button class="md:hidden flex items-center js-menu" aria-label="Menu" type="button" aria-controls="head-nav" aria-expanded="false">
        <div class="hamburger">
          <span class="hamburger_1"></span>
          <span class="hamburger_2"></span>
          <span class="hamburger_3"></span>
        </div>
      </button>
    </div>
  </div>
</div>
<div class="sy-page sy-container flex mx-auto">
  <aside id="lside" class="sy-lside md:w-72 md:shrink-0 print:hidden">
    <div class="sy-lside-inner md:sticky">
      <div class="sy-scrollbar p-6">
        <div class="globaltoc" data-expand-depth="0"><ul class="current">
<li class="toctree-l1"><a class="reference internal" href="data/index.html">Data and datasets</a><ul>
<li class="toctree-l2"><a class="reference internal" href="data/atom_data.html">Molecular Data Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="data/fixed_datasets.html">Fixed datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="data/h5_datasets.html">H5 Dataset</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="geometry/index.html">Geometry and Neighbor lists</a><ul>
<li class="toctree-l2"><a class="reference internal" href="geometry/internal_coordinates.html">Internal Coordinate Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="geometry/neighbor_list.html">Neighbor Lists</a></li>
<li class="toctree-l2"><a class="reference internal" href="geometry/topology.html">Topology Utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="geometry/statistics.html">Statistics Utilities</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="models/index.html">Model utilities</a><ul>
<li class="toctree-l2"><a class="reference internal" href="models/priors.html">Priors</a></li>
<li class="toctree-l2"><a class="reference internal" href="models/cutoffs.html">Cutoff Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="models/rbfs.html">Radial Basis Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="models/mlip.html">MLIPs</a></li>
<li class="toctree-l2"><a class="reference internal" href="models/wrappers.html">Gradient/sum wrappers</a></li>
<li class="toctree-l2"><a class="reference internal" href="models/losses.html">Loss Functions</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="simulation/index.html">Simulations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="simulation/langevin.html">Langevin Simulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="simulation/overdamped.html">Overdamped Simulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="simulation/parallel_tempering.html">Parallel Tempering Simulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="simulation/utils.html">Simulation utilities</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="misc/index.html">Miscellaneous utilities</a><ul>
<li class="toctree-l2"><a class="reference internal" href="misc/coarse_graining.html">Coarse Graining</a></li>
<li class="toctree-l2"><a class="reference internal" href="misc/utils.html">General utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="misc/mol_utils.html">Molecular utilities</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="dev/index.html">Developer Documentation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="dev/doc.html">Documentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="dev/tests.html">Tests</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>

        </div>
      </div>
    </div>
  </aside>
  <div class="lside-overlay js-menu" role="button" aria-label="Close left sidebar" aria-controls="lside" aria-expanded="false"></div>
  <aside id="rside" class="sy-rside pb-3 w-64 shrink-0 order-last">
    <button class="rside-close js-menu xl:hidden" aria-label="Close Table of Contents" type="button" aria-controls="rside" aria-expanded="false">
      <i class="i-lucide close"></i>
    </button>
    <div class="sy-scrollbar sy-rside-inner px-6 xl:top-16 xl:sticky xl:pl-0 pt-6 pb-4"><div class="localtoc"><h3>On this page</h3><ul>
<li><a class="reference internal" href="#extensions-for-using-pytorch-lightning">Extensions for using Pytorch Lightning</a><ul>
<li><a class="reference internal" href="#mlcg.pl.DataModule"><code class="docutils literal notranslate"><span class="pre">DataModule</span></code></a><ul>
<li><a class="reference internal" href="#mlcg.pl.DataModule.prepare_data"><code class="docutils literal notranslate">prepare_data()</span></code></a></li>
<li><a class="reference internal" href="#mlcg.pl.DataModule.setup"><code class="docutils literal notranslate">setup()</span></code></a></li>
<li><a class="reference internal" href="#mlcg.pl.DataModule.test_dataloader"><code class="docutils literal notranslate">test_dataloader()</span></code></a></li>
<li><a class="reference internal" href="#mlcg.pl.DataModule.train_dataloader"><code class="docutils literal notranslate">train_dataloader()</span></code></a></li>
<li><a class="reference internal" href="#mlcg.pl.DataModule.val_dataloader"><code class="docutils literal notranslate">val_dataloader()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#mlcg.pl.PLModel"><code class="docutils literal notranslate"><span class="pre">PLModel</span></code></a><ul>
<li><a class="reference internal" href="#mlcg.pl.PLModel.on_train_epoch_end"><code class="docutils literal notranslate">on_train_epoch_end()</span></code></a></li>
<li><a class="reference internal" href="#mlcg.pl.PLModel.on_train_epoch_start"><code class="docutils literal notranslate">on_train_epoch_start()</span></code></a></li>
<li><a class="reference internal" href="#mlcg.pl.PLModel.on_validation_epoch_end"><code class="docutils literal notranslate">on_validation_epoch_end()</span></code></a></li>
<li><a class="reference internal" href="#mlcg.pl.PLModel.test_step"><code class="docutils literal notranslate">test_step()</span></code></a></li>
<li><a class="reference internal" href="#mlcg.pl.PLModel.training_step"><code class="docutils literal notranslate">training_step()</span></code></a></li>
<li><a class="reference internal" href="#mlcg.pl.PLModel.validation_step"><code class="docutils literal notranslate">validation_step()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#mlcg.pl.LightningCLI"><code class="docutils literal notranslate"><span class="pre">LightningCLI</span></code></a><ul>
<li><a class="reference internal" href="#mlcg.pl.LightningCLI.parse_arguments"><code class="docutils literal notranslate">parse_arguments()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#scripts">Scripts</a></li>
</ul>
</div><div id="ethical-ad-placement" data-ea-publisher="readthedocs"></div></div>
  </aside>
  <div class="rside-overlay js-menu" role="button" aria-label="Close Table of Contents" aria-controls="rside" aria-expanded="false"></div>
  <main class="sy-main w-full max-sm:max-w-full print:pt-6">
<div class="sy-breadcrumbs" role="navigation">
  <div class="sy-breadcrumbs-inner flex items-center">
    <div class="md:hidden mr-3">
      <button class="js-menu" aria-label="Menu" type="button" aria-controls="lside" aria-expanded="false">
        <i class="i-lucide menu"></i>
      </button>
    </div>
    <ol class="flex-1" itemscope itemtype="https://schema.org/BreadcrumbList"><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <a itemprop="item" href="index.html"><span itemprop="name">mlcg</span></a>
        <span>/</span>
        <meta itemprop="position" content="1" />
      </li><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <strong itemprop="name">Training</strong>
        <meta itemprop="position" content="2" />
      </li></ol>
    <div class="xl:hidden ml-1">
      <button class="js-menu" aria-label="Show table of contents" type="button" aria-controls="rside"
        aria-expanded="false">
        <i class="i-lucide outdent"></i>
      </button>
    </div>
  </div>
</div><div class="flex flex-col break-words justify-between">
      <div class="relative min-w-0 max-w-6xl px-6 pb-6 pt-8 xl:px-12">
  <article class="yue" role="main">
          <section id="training">
<h1>Training<a class="headerlink" href="#training" title="Link to this heading">¶</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">mlcg</span></code> provides some tools to train its models in the <code class="docutils literal notranslate"><span class="pre">scripts</span></code> folder and some example input
files such as <code class="docutils literal notranslate"><span class="pre">examples/train_schnet.yaml</span></code>. The training is defined
using the <a class="reference external" href="https://pytorch-lightning.readthedocs.io/en/latest/">pytorch-lightning</a> package and
especially its <a class="reference external" href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_cli.html">cli</a> utilities.</p>
<section id="extensions-for-using-pytorch-lightning">
<h2>Extensions for using Pytorch Lightning<a class="headerlink" href="#extensions-for-using-pytorch-lightning" title="Link to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="mlcg.pl.DataModule">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">mlcg.pl.</span></span><span class="sig-name descname"><span class="pre">DataModule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_dir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">splits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inference_batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loading_stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_local_copy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mlcg/pl/data.html#DataModule"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mlcg.pl.DataModule" title="Link to this definition">¶</a></dt>
<dd><p>PL interface to train with datasets defined in mlcg.datasets.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">InMemoryDataset</span></code></span>) – a dataset from mlcg.datasets (or following the API of <cite>torch_geometric.data.InMemoryDataset</cite>)</p></li>
<li><p><strong>log_dir</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></span>) – where to store the data that might be produced during training.</p></li>
<li><p><strong>val_ratio</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></span>) – fraction of the dataset used for validation</p></li>
<li><p><strong>test_ratio</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></span>) – fraction of the dataset used for testing</p></li>
<li><p><strong>splits</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></span>) – filename of a file containing the indices for training, validation, and testing. It should be compatible with <cite>np.load</cite> and contain the fields <cite>‘idx_train’</cite>, <cite>‘idx_val’</cite>, and <cite>‘idx_test’</cite>.
If None then the dataset is split randomly using the <cite>val_ratio</cite> and <cite>test_ratio</cite>.</p></li>
<li><p><strong>batch_size</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – number of structure to include in each training batches.</p></li>
<li><p><strong>inference_batch_size</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – number of structure to include in each validation/training batches.</p></li>
<li><p><strong>num_workers;</strong> – number of <cite>cpu</cite> used for loading the dataset (see <a class="reference external" href="https://pytorch-lightning.readthedocs.io/en/stable/guides/speed.html?highlight=num_workers#num-workers">here</a> for more details).</p></li>
<li><p><strong>loading_stride</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – stride used to subselect the dataset. Useful parameter for debugging purposes.</p></li>
<li><p><strong>save_local_copy</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – saves the input dataset in log_dir</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="mlcg.pl.DataModule.prepare_data">
<span class="sig-name descname"><span class="pre">prepare_data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mlcg/pl/data.html#DataModule.prepare_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mlcg.pl.DataModule.prepare_data" title="Link to this definition">¶</a></dt>
<dd><p>Download, preprocess dataset, etc.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlcg.pl.DataModule.setup">
<span class="sig-name descname"><span class="pre">setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mlcg/pl/data.html#DataModule.setup"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mlcg.pl.DataModule.setup" title="Link to this definition">¶</a></dt>
<dd><p>Called at the beginning of fit (train + validate), validate, test, or predict. This is a good hook when you
need to build models dynamically or adjust something about them. This hook is called on every process when
using DDP.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>stage</strong> – either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span data-line="1"><span class="k">class</span><span class="w"> </span><span class="nc">LitModel</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
</span><span data-line="2">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span data-line="3">        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="kc">None</span>
</span><span data-line="4">
</span><span data-line="5">    <span class="k">def</span><span class="w"> </span><span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span data-line="6">        <span class="n">download_data</span><span class="p">()</span>
</span><span data-line="7">        <span class="n">tokenize</span><span class="p">()</span>
</span><span data-line="8">
</span><span data-line="9">        <span class="c1"># don&#39;t do this</span>
</span><span data-line="10">        <span class="bp">self</span><span class="o">.</span><span class="n">something</span> <span class="o">=</span> <span class="k">else</span>
</span><span data-line="11">
</span><span data-line="12">    <span class="k">def</span><span class="w"> </span><span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stage</span><span class="p">):</span>
</span><span data-line="13">        <span class="n">data</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</span><span data-line="14">        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
</span></pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlcg.pl.DataModule.test_dataloader">
<span class="sig-name descname"><span class="pre">test_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mlcg/pl/data.html#DataModule.test_dataloader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mlcg.pl.DataModule.test_dataloader" title="Link to this definition">¶</a></dt>
<dd><p>An iterable or collection of iterables specifying test samples.</p>
<p>For more information about multiple dataloaders, see this <span class="xref std std-ref">section</span>.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#mlcg.pl.DataModule.prepare_data" title="mlcg.pl.DataModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#mlcg.pl.DataModule.setup" title="mlcg.pl.DataModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">test()</span></code></p></li>
<li><p><a class="reference internal" href="#mlcg.pl.DataModule.prepare_data" title="mlcg.pl.DataModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#mlcg.pl.DataModule.setup" title="mlcg.pl.DataModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a test dataset and a <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code>, you don’t need to implement
this method.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlcg.pl.DataModule.train_dataloader">
<span class="sig-name descname"><span class="pre">train_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mlcg/pl/data.html#DataModule.train_dataloader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mlcg.pl.DataModule.train_dataloader" title="Link to this definition">¶</a></dt>
<dd><p>An iterable or collection of iterables specifying training samples.</p>
<p>For more information about multiple dataloaders, see this <span class="xref std std-ref">section</span>.</p>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id1"><span class="problematic" id="id2">:paramref:`~pytorch_lightning.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#mlcg.pl.DataModule.prepare_data" title="mlcg.pl.DataModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#mlcg.pl.DataModule.setup" title="mlcg.pl.DataModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p><a class="reference internal" href="#mlcg.pl.DataModule.prepare_data" title="mlcg.pl.DataModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#mlcg.pl.DataModule.setup" title="mlcg.pl.DataModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlcg.pl.DataModule.val_dataloader">
<span class="sig-name descname"><span class="pre">val_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mlcg/pl/data.html#DataModule.val_dataloader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mlcg.pl.DataModule.val_dataloader" title="Link to this definition">¶</a></dt>
<dd><p>An iterable or collection of iterables specifying validation samples.</p>
<p>For more information about multiple dataloaders, see this <span class="xref std std-ref">section</span>.</p>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id3"><span class="problematic" id="id4">:paramref:`~pytorch_lightning.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>It’s recommended that all data downloads and preparation happen in <a class="reference internal" href="#mlcg.pl.DataModule.prepare_data" title="mlcg.pl.DataModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">validate()</span></code></p></li>
<li><p><a class="reference internal" href="#mlcg.pl.DataModule.prepare_data" title="mlcg.pl.DataModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#mlcg.pl.DataModule.setup" title="mlcg.pl.DataModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a validation dataset and a <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code>, you don’t need to
implement this method.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mlcg.pl.PLModel">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">mlcg.pl.</span></span><span class="sig-name descname"><span class="pre">PLModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mlcg/pl/model.html#PLModel"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mlcg.pl.PLModel" title="Link to this definition">¶</a></dt>
<dd><p>PL interface to train with models defined in <span class="xref std std-ref">mlcg.nn</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></span>) – instance of a model class from <span class="xref std std-ref">mlcg.nn</span>.</p></li>
<li><p><strong>loss</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="models/losses.html#mlcg.nn.losses.Loss" title="mlcg.nn.losses.Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">Loss</span></code></a></span>) – instance of <span class="xref std std-ref">mlcg.nn.Loss</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="mlcg.pl.PLModel.on_train_epoch_end">
<span class="sig-name descname"><span class="pre">on_train_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mlcg/pl/model.html#PLModel.on_train_epoch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mlcg.pl.PLModel.on_train_epoch_end" title="Link to this definition">¶</a></dt>
<dd><p>Called in the training loop at the very end of the epoch.</p>
<p>To access all batch outputs at the end of the epoch, you can cache step outputs as an attribute of the
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> and access them in this hook:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span data-line="1"><span class="k">class</span><span class="w"> </span><span class="nc">MyLightningModule</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
</span><span data-line="2">    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span data-line="3">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span data-line="4">        <span class="bp">self</span><span class="o">.</span><span class="n">training_step_outputs</span> <span class="o">=</span> <span class="p">[]</span>
</span><span data-line="5">
</span><span data-line="6">    <span class="k">def</span><span class="w"> </span><span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span data-line="7">        <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
</span><span data-line="8">        <span class="bp">self</span><span class="o">.</span><span class="n">training_step_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</span><span data-line="9">        <span class="k">return</span> <span class="n">loss</span>
</span><span data-line="10">
</span><span data-line="11">    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span data-line="12">        <span class="c1"># do something with all training_step outputs, for example:</span>
</span><span data-line="13">        <span class="n">epoch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_step_outputs</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span><span data-line="14">        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;training_epoch_mean&quot;</span><span class="p">,</span> <span class="n">epoch_mean</span><span class="p">)</span>
</span><span data-line="15">        <span class="c1"># free up the memory</span>
</span><span data-line="16">        <span class="bp">self</span><span class="o">.</span><span class="n">training_step_outputs</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
</span></pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlcg.pl.PLModel.on_train_epoch_start">
<span class="sig-name descname"><span class="pre">on_train_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mlcg/pl/model.html#PLModel.on_train_epoch_start"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mlcg.pl.PLModel.on_train_epoch_start" title="Link to this definition">¶</a></dt>
<dd><p>Called in the training loop at the very beginning of the epoch.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlcg.pl.PLModel.on_validation_epoch_end">
<span class="sig-name descname"><span class="pre">on_validation_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mlcg/pl/model.html#PLModel.on_validation_epoch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mlcg.pl.PLModel.on_validation_epoch_end" title="Link to this definition">¶</a></dt>
<dd><p>Called in the validation loop at the very end of the epoch.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlcg.pl.PLModel.test_step">
<span class="sig-name descname"><span class="pre">test_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mlcg/pl/model.html#PLModel.test_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mlcg.pl.PLModel.test_step" title="Link to this definition">¶</a></dt>
<dd><p>Operates on a single batch of data from the test set. In this step you’d normally generate examples or
calculate anything of interest such as accuracy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> – The output of your data iterable, normally a <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>.</p></li>
<li><p><strong>batch_idx</strong> – The index of this batch.</p></li>
<li><p><strong>dataloader_idx</strong> – The index of the dataloader that produced this batch.
(only if multiple dataloaders used)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> - The loss tensor</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dict</span></code> - A dictionary. Can include any keys, but must include the key <code class="docutils literal notranslate"><span class="pre">'loss'</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Skip to the next batch.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span data-line="1"><span class="c1"># if you have one test dataloader:</span>
</span><span data-line="2"><span class="k">def</span><span class="w"> </span><span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span> <span class="o">...</span>
</span><span data-line="3">
</span><span data-line="4">
</span><span data-line="5"><span class="c1"># if you have multiple test dataloaders:</span>
</span><span data-line="6"><span class="k">def</span><span class="w"> </span><span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span> <span class="o">...</span>
</span></pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span data-line="1"><span class="c1"># CASE 1: A single test dataset</span>
</span><span data-line="2"><span class="k">def</span><span class="w"> </span><span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
</span><span data-line="3">    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
</span><span data-line="4">
</span><span data-line="5">    <span class="c1"># implement your own</span>
</span><span data-line="6">    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span data-line="7">    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span data-line="8">
</span><span data-line="9">    <span class="c1"># log 6 example images</span>
</span><span data-line="10">    <span class="c1"># or generated text... or whatever</span>
</span><span data-line="11">    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
</span><span data-line="12">    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
</span><span data-line="13">    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span data-line="14">
</span><span data-line="15">    <span class="c1"># calculate acc</span>
</span><span data-line="16">    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span data-line="17">    <span class="n">test_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>
</span><span data-line="18">
</span><span data-line="19">    <span class="c1"># log the outputs!</span>
</span><span data-line="20">    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">&#39;test_loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc</span><span class="p">})</span>
</span></pre></div>
</div>
<p>If you pass in multiple test dataloaders, <a class="reference internal" href="#mlcg.pl.PLModel.test_step" title="mlcg.pl.PLModel.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will have an additional argument. We recommend
setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span data-line="1"><span class="c1"># CASE 2: multiple test dataloaders</span>
</span><span data-line="2"><span class="k">def</span><span class="w"> </span><span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
</span><span data-line="3">    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
</span><span data-line="4">    <span class="o">...</span>
</span></pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need to test you don’t need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <a class="reference internal" href="#mlcg.pl.PLModel.test_step" title="mlcg.pl.PLModel.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> is called, the model has been put in eval mode and
PyTorch gradients have been disabled. At the end of the test epoch, the model goes back
to training mode and gradients are enabled.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlcg.pl.PLModel.training_step">
<span class="sig-name descname"><span class="pre">training_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mlcg/pl/model.html#PLModel.training_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mlcg.pl.PLModel.training_step" title="Link to this definition">¶</a></dt>
<dd><p>Here you compute and return the training loss and some additional metrics for e.g. the progress bar or
logger.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> – The output of your data iterable, normally a <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>.</p></li>
<li><p><strong>batch_idx</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – The index of this batch.</p></li>
<li><p><strong>dataloader_idx</strong> – The index of the dataloader that produced this batch.
(only if multiple dataloaders used)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> - The loss tensor</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dict</span></code> - A dictionary which can include any keys, but must include the key <code class="docutils literal notranslate"><span class="pre">'loss'</span></code> in the case of
automatic optimization.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - In automatic optimization, this will skip to the next batch (but is not supported for
multi-GPU, TPU, or DeepSpeed). For manual optimization, this has no special meaning, as returning
the loss is not required.</p></li>
</ul>
</p>
</dd>
</dl>
<p>In this step you’d normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span data-line="1"><span class="k">def</span><span class="w"> </span><span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
</span><span data-line="2">    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">batch</span>
</span><span data-line="3">    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span data-line="4">    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</span><span data-line="5">    <span class="k">return</span> <span class="n">loss</span>
</span></pre></div>
</div>
<p>To use multiple optimizers, you can switch to ‘manual optimization’ and control their stepping:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span data-line="1"><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span data-line="2">    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span data-line="3">    <span class="bp">self</span><span class="o">.</span><span class="n">automatic_optimization</span> <span class="o">=</span> <span class="kc">False</span>
</span><span data-line="4">
</span><span data-line="5">
</span><span data-line="6"><span class="c1"># Multiple optimizers (e.g.: GANs)</span>
</span><span data-line="7"><span class="k">def</span><span class="w"> </span><span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
</span><span data-line="8">    <span class="n">opt1</span><span class="p">,</span> <span class="n">opt2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">()</span>
</span><span data-line="9">
</span><span data-line="10">    <span class="c1"># do training_step with encoder</span>
</span><span data-line="11">    <span class="o">...</span>
</span><span data-line="12">    <span class="n">opt1</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span><span data-line="13">    <span class="c1"># do training_step with decoder</span>
</span><span data-line="14">    <span class="o">...</span>
</span><span data-line="15">    <span class="n">opt2</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When <code class="docutils literal notranslate"><span class="pre">accumulate_grad_batches</span></code> &gt; 1, the loss returned here will be automatically
normalized by <code class="docutils literal notranslate"><span class="pre">accumulate_grad_batches</span></code> internally.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlcg.pl.PLModel.validation_step">
<span class="sig-name descname"><span class="pre">validation_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mlcg/pl/model.html#PLModel.validation_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mlcg.pl.PLModel.validation_step" title="Link to this definition">¶</a></dt>
<dd><p>The order of separate validation losses (bearing the name <cite>dataloader_idx_?</cite>) will
be alphabetically ascending with respect to the Metaset names in the multi-metaset scenario.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mlcg.pl.LightningCLI">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">mlcg.pl.</span></span><span class="sig-name descname"><span class="pre">LightningCLI</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">datamodule_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_config_callback</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;class</span> <span class="pre">'pytorch_lightning.cli.SaveConfigCallback'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_config_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainer_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">&lt;class</span> <span class="pre">'pytorch_lightning.trainer.trainer.Trainer'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainer_defaults</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed_everything_default</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parser_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">subclass_mode_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">subclass_mode_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">run</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">auto_configure_optimizers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mlcg/pl/cli.html#LightningCLI"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mlcg.pl.LightningCLI" title="Link to this definition">¶</a></dt>
<dd><p>Command line interface for training a model with pytorch lightning.</p>
<p>It adds a few functionalities to <cite>pytorch_lightning.utilities.cli.LightningCLI</cite>.</p>
<ul class="simple">
<li><p>register torch optimizers and lr_scheduler so that they can be specified</p></li>
</ul>
<p>in the configuration file. Note that only single (optimizer,lr_scheduler)
can be specified like that and more complex patterns should be implemented
in the pytorch_lightning model definition (child of <cite>pytorch_lightning.
LightningModule</cite>). see <a class="reference external" href="https://pytorch-lightning.readthedocs.io/en/1.4.9/common/lightning_cli.html#optimizers-and-learning-rate-schedulers">doc</a>
for more details.</p>
<ul class="simple">
<li><p>link manually some arguments related to the definition of the work directory. If</p></li>
</ul>
<p><cite>default_root_dir</cite> argument of <cite>pytorch_lightning.Trainer</cite> is set and the
<cite>save_dir</cite> / <cite>log_dir</cite> / <cite>dirpath</cite> argument of <cite>loggers</cite> / <cite>data</cite> / <cite>callbacks</cite> is
set to <cite>default_root_dir</cite> then they will be set to the value of
<cite>default_root_dir</cite> / <cite>default_root_dir/data</cite> / <cite>default_root_dir/ckpt</cite>.</p>
<dl class="py method">
<dt class="sig sig-object py" id="mlcg.pl.LightningCLI.parse_arguments">
<span class="sig-name descname"><span class="pre">parse_arguments</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parser</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mlcg/pl/cli.html#LightningCLI.parse_arguments"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mlcg.pl.LightningCLI.parse_arguments" title="Link to this definition">¶</a></dt>
<dd><p>Parses command line arguments and stores it in self.config</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="scripts">
<h2>Scripts<a class="headerlink" href="#scripts" title="Link to this heading">¶</a></h2>
<p>Scripts that are using <code class="docutils literal notranslate"><span class="pre">LightningCLI</span></code> have many convinient built in
<a class="reference external" href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_cli.html#lightningcli">functionalities</a>
such as a detailed helper</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span data-line="1">python<span class="w"> </span>scripts/mlcg-train.py<span class="w"> </span>--help
</span><span data-line="2">python<span class="w"> </span>scripts/mlcg-train_h5.py<span class="w"> </span>--help
</span></pre></div>
</div>
</section>
</section>

        </article><button class="back-to-top" type="button">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
  </svg>
  <span>Back to top</span>
</button><div class="navigation flex print:hidden"><div class="navigation-prev">
    <a href="models/losses.html">
      <i class="i-lucide chevron-left"></i>
      <div class="page-info">
        <span>Previous</span><div class="title">Loss Functions</div></div>
    </a>
  </div><div class="navigation-next">
    <a href="simulation/index.html">
      <div class="page-info">
        <span>Next</span>
        <div class="title">Simulations</div>
      </div>
      <i class="i-lucide chevron-right"></i>
    </a>
  </div></div></div>
    </div>
  </main>
</div>
<footer class="sy-foot">
  <div class="sy-foot-inner sy-container mx-auto">
    <div class="sy-foot-reserved md:flex justify-between items-center">
      <div class="sy-foot-copyright"><p>2026, Clementi Group</p>
  
  <p>
    Made with
    
    <a href="https://www.sphinx-doc.org/">Sphinx</a> and
    
    <a href="https://shibuya.lepture.com">Shibuya theme</a>.
  </p>
</div>
      <div class="sy-foot-socials"></div>
    </div>
  </div>
</footer>
      <script src="_static/documentation_options.js?v=360bc84d"></script>
      <script src="_static/doctools.js?v=fd6eb6e6"></script>
      <script src="_static/sphinx_highlight.js?v=6ffebe34"></script>
      <script src="_static/shibuya.js?v=cac61aee"></script></body>
</html>