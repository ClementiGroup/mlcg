seed_everything: 19384
# define the training
trainer:
  # Work directory for the session
  default_root_dir: /local_scratch/musil/tests/t1
  # set a limit to the training (#epochs and/or time limit)
  max_epochs: 6
  max_time: null
  resume_from_checkpoint: null
  profiler: null

  # Training on one/multiple GPU
  # accelerator: 'ddp'
  # gpus: 2
  # num_nodes: 1

  # Training on the CPU
  accelerator:
    class_path: pytorch_lightning.accelerators.CPUAccelerator
    init_args:
      precision_plugin:
        class_path: pytorch_lightning.plugins.DoublePrecisionPlugin
        init_args: {}
      training_type_plugin:
        class_path: mlcg.pl.SingleDevicePlugin
        init_args:
          device: 'cpu'
  benchmark: false
  logger:
    - class_path: pytorch_lightning.loggers.TensorBoardLogger
      init_args:
        # save_dir will be set to default_root_dir
        save_dir: default_root_dir
        name: tensorboard
        version: ''
    - class_path: pytorch_lightning.loggers.CSVLogger
      init_args:
        # save_dir will be set to default_root_dir
        save_dir: default_root_dir
        name: ''
        version: ''
  checkpoint_callback: true
  callbacks:
    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        monitor: 'validation_loss'
        patience: 10
    # Save states of the model a regular interval
    - class_path: pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint
      init_args:
        # checkpoints will be saved to default_root_dir/ckpt
        dirpath: default_root_dir
        monitor: validation_loss
        save_top_k: -1
        every_n_epochs: 2
        filename: '{epoch}-{validation_loss:.4f}'
        save_last: true
  log_every_n_steps: 10
  gradient_clip_val: 0
  gradient_clip_algorithm: norm
  log_gpu_memory: null
  track_grad_norm: inf
  check_val_every_n_epoch: 1
  fast_dev_run: false
  accumulate_grad_batches: 1
  precision: 64
  weights_summary: top
  deterministic: true
  auto_lr_find: false
  terminate_on_nan: true
  amp_backend: native
  amp_level: O2
# define the model and training objectives using mlcg.pl.PLModel
model:
  loss:
    class_path: mlcg.nn.Loss
    init_args:
      # list of losses to optimize
      losses:
        - class_path: mlcg.nn.ForceRMSE
          init_args:
            force_kwd: forces
  # model specifications
  model:
    class_path: mlcg.nn.GradientsOut
    init_args:
      targets: forces
      model:
        class_path: mlcg.nn.schnet.SimpleSchNet
        init_args:
          hidden_channels: 128
          max_z: 50
          num_filters: 128
          num_interactions: 2
          output_hidden_layer_widths:
            - 100
            - 60
            - 30
          activation:
            class_path: torch.nn.Tanh
            init_args: {}
          max_num_neighbors: 20
          aggr: "add"
          cutoff:
            class_path: mlcg.nn.CosineCutoff
            init_args:
              cutoff_upper: 15
          rbf_layer:
            class_path: mlcg.nn.ExpNormalBasis
            init_args:
              cutoff_upper: 15
              num_rbf: 30
              trainable: false
  # necessary parameters when using ReduceLROnPlateau
  monitor: validation_loss
  interval: 'epoch'
  frequency: 1
optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 0.0001
    weight_decay: 0
lr_scheduler:
  class_path: torch.optim.lr_scheduler.ReduceLROnPlateau
  init_args:
    factor: 0.1
    patience: 10
    min_lr: 1e-5
# define the dataset through mlcg.pl.DataModule
data:
  dataset:
    class_path: mlcg.datasets.ChignolinDataset
    init_args:
      root: /local_scratch/musil/datasets/test
  log_dir: default_root_dir
  val_ratio: 0.1
  test_ratio: 0.1
  batch_size: 1024
  inference_batch_size: 1024
  num_workers: 2
  train_stride: 1
  save_local_copy: false
